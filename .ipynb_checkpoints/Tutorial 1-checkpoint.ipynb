{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo pip3 install sklearn pandas imgaug matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnist jest to zbiór 70000 pisanych cyfr zapisanych jako obrazki 28px*28px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", data_home=\"./mnist\", cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(mnist.data).reshape(70000,28,28)[102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMNIST (Fashion-MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FMNIST jest to zbiór 70000 zdjęć produktów firmy Zalando jako obrazki 28px*28px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = fetch_openml(\"Fashion-MNIST\", data_home=\"./fmnist\", cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(fmnist.data).reshape(70000,28,28)[102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR_10 jest to zbiór 60000 obrazków 32px*32px (6000 na klasę) w 10 różnych klasach (samolot, samochód, ptak, kot, jeleń, pies, żaba, koń, statek, ciężarówka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = fetch_openml(\"CIFAR_10\", data_home=\"./cifar10\", cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(np.array(cifar10.data).reshape(60000,3,32,32)[4].astype(int), (1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmallNorb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SmallNorb to zbiór obrazów do eksperymentów z rozpoznawaniem obrazów 3D. Składa się z 50 zabawek należących do 5 różnych kategorii. Dla każdej zabawki jest to zestaw zdjęć pod różnymi warunkami światła, podniesieniami i azymutami"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![smallNorb.png](./smallNorb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TNG to zbiór wszystkich odcinków Star Trek TNG. Każdy rząd zawiera osobną wypowiedź albo opis ze scenariusza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tng = pd.read_csv(\"./TNG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communities and Crime Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = pd.read_csv(\"./crime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wybieranie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybranie `n` pierwszych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybranie `n` losowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.sample(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podział danych testowych z użyciem sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(crime.drop(columns='nonViolPerPop'), crime['nonViolPerPop'], test_size=0.2)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_replaced = crime.set_index([\"communityname\", \"state\", \"countyCode\", \"communityCode\", \"fold\"]).replace('?', np.NaN)\n",
    "crime_replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uzupełnienie brakujących danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "crime_filled = pd.DataFrame(imputer.fit_transform(crime_replaced),columns=crime_replaced.columns,index=crime_replaced.index)\n",
    "crime_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalowanie z użyciem wartości maksymalnej i minimalnej."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![minmax.png](./minmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "crime_scaled = pd.DataFrame(min_max_scaler.fit_transform(crime_filled),columns=crime_filled.columns,index=crime_filled.index)\n",
    "crime_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standaryzacja - skalowanie mające na celu przesunięcie rozkładu tak, aby średnia była równa 0, a odchylenie standardowe 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![standarization.png](./standarization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "crime_standardized = pd.DataFrame(scaler.fit_transform(crime_filled),columns=crime_filled.columns,index=crime_filled.index)\n",
    "crime_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_standardized.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacja - przeskalowanie każdej obserwacji do długości 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![normalization.png](./normalization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "crime_normalized = pd.DataFrame(normalizer.fit_transform(crime_filled),columns=crime_filled.columns,index=crime_filled.index)\n",
    "crime_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytm k-NN:\n",
    " * Liczymy odległość wszystkich pomiarów od rozważanego\n",
    " * Wybieramy k najbliższych pomiarów do naszego\n",
    " * Przypisujemy rozważanemu pomiarowi taką etykietę, jaką ma najwięcej pomiarów z tych k wybranych\n",
    " \n",
    "![kNN.png](./kNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-NN z użyciem sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(mnist.data.reshape((70000, 28*28))[:500], mnist.target[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.predict(mnist.data.reshape((70000, 28*28))[1000:1005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mnist.data.reshape((70000, 28, 28))[1002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementacje KNeighborsClassifier(parametr `algorithm` konstruktora):\n",
    " * brute - dobry dla danych rzadkich, za każdym razem liczy odległości do wszystkich punktów\n",
    " * ball_tree i kd_tree - struktury reprezentujące podział wieliwymiarowych przestrzeni (mają strukturę drzew binarnych, w których każdy element, który nie jest liściem, można interpretować jako podział przestrzeni na dwie części). Mają podobne benchmarki. (Ball Tree zwykle nieco szybciej się uczy, natomiast k-d Tree szybciej oblicza predykcje)\n",
    " * auto - ustala najlepszy algorytm dla podanych danych i go stosuje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biblioteka FAISS https://github.com/facebookresearch/faiss\n",
    "\n",
    "Biblioteka stosowana do szybkiego rozwiązywania problemów wyszykiwania podobieństwa i grupowania. Napisana głównie w C++, z dowiązaniami do Pytohna. Najprzydatniejsze algorytmy są zaimplementowanie do użycia na GPU z wykorzystaniem CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, KernelDensity\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "wines = load_wine()\n",
    "rnc = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation** to metoda oceny modelu statystycznego. Jest lepsza od pozostałych metod, ponieważ zwraca informacje o tym, jak dobrze zachowa się nauczona maszyna podczas klasyfikacji nowych elementów. Polega na tym, że nie wykorzystuje się całego zbioru danych do nauki. Należy podzielić zbiór na dwie części: jedną wykorzystać do nauki, a drugą do testu nauczonej maszyny. Test weryfikuje, jak dobrze nauczona maszyna klasyfikuje elementy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Istnieją różne ulepszenia bazowej idei kroswalidacji. Jedną z nich jest **K-fold cross-validation**. Zamiast dzielić zbiór na dwie części, dzielimy go na K części. Wykonujemy procedurę nauki K razy, za każdym razem wybieramy jeden z K podzbiorów jako zbiór testowy, natomiast pozostałe K-1 zbiorów są użyte do nauki. Następnie obliczamy średnią z wszystkich testów, co daje nam ocenę."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przypadkiem szczególnym K-fold cross-validation jest metoda **leave-one-out**. W tym przypadku K jest równe liczności zbioru danych, co powoduje, że używamy do nauki wszystkich elementów poza jednym, a następnie przeprowadzamy test tylko na jednym elemencie. Ze wszystkich N testów wyciągamy średnią. Na początku wydaje się to niewydajną metodą, ponieważ należy przeprowadzić algorytm uczenia N razy, jednak istnieją usprawnienia, które sprawiają, że przeprowadzenie ewaluacji nie jest tak kosztowne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation - wariant podstawowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(rnc, iris.data, iris.target, cv = 2)\n",
    "print(\"Liczba iteracji: \", len(results))\n",
    "print(\"Średnia trafność: {:.2f}\".format(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "results = cross_val_score(rnc, iris.data, iris.target, cv = kfold)\n",
    "print(\"Liczba iteracji: \", len(results))\n",
    "print(\"Średnia trafność: {:.2f}\".format(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "\n",
    "results = cross_val_score(rnc, wines.data, wines.target, cv = loo)\n",
    "print(\"Liczba iteracji: \", len(results))\n",
    "print(\"Średnia trafność: {:.2f}\".format(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenghts = []\n",
    "results = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
    "for i in range(len(X_train) - 2):\n",
    "    x_train_trimmed = X_train[i:][:]\n",
    "    y_train_trimmed = y_train[i:][:]\n",
    "    nc = KNeighborsClassifier(n_neighbors = 3)\n",
    "    model = nc.fit(x_train_trimmed, y_train_trimmed)\n",
    "    lenghts += [len(x_train_trimmed)]\n",
    "    results += [model.score(X_test, y_test)]\n",
    "plt.plot(lenghts, results)\n",
    "plt.ylabel(\"Precyzja klasyfikacji\")\n",
    "plt.xlabel(\"Ilość elementów w zbiorze uczącym\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenghts = []\n",
    "results = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
    "for i in range(len(X_train) - 4):\n",
    "    x_train_trimmed = X_train[i:][:]\n",
    "    y_train_trimmed = y_train[i:][:]\n",
    "    nc = KNeighborsClassifier(n_neighbors = 5)\n",
    "    model = nc.fit(x_train_trimmed, y_train_trimmed)\n",
    "    lenghts += [len(x_train_trimmed)]\n",
    "    results += [accuracy_score(y_test, nc.predict(X_test))]\n",
    "plt.plot(lenghts, results)\n",
    "plt.ylabel(\"Precyzja klasyfikacji\")\n",
    "plt.xlabel(\"Ilość elementów w zbiorze uczącym\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenghts = []\n",
    "results = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
    "for i in range(len(X_train)):\n",
    "    x_train_trimmed = X_train[i:][:]\n",
    "    y_train_trimmed = y_train[i:][:]\n",
    "    nc = KNeighborsClassifier(n_neighbors = 1)\n",
    "    model = nc.fit(x_train_trimmed, y_train_trimmed)\n",
    "    lenghts += [len(x_train_trimmed)]\n",
    "    results += [accuracy_score(y_test, nc.predict(X_test))]\n",
    "plt.plot(lenghts, results)\n",
    "plt.ylabel(\"Precyzja klasyfikacji\")\n",
    "plt.xlabel(\"Ilość elementów w zbiorze uczącym\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmiana k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
    "for j in range(1000):\n",
    "    for i in range(1,20,1):\n",
    "        x_train_trimmed = X_train[:40][:]\n",
    "        y_train_trimmed = y_train[:40][:]\n",
    "        nc = KNeighborsClassifier(n_neighbors = i)\n",
    "        model = nc.fit(x_train_trimmed, y_train_trimmed)\n",
    "        results[i] += [accuracy_score(y_test, nc.predict(X_test))]\n",
    "for k,v in results.items():\n",
    "    results[k] = sum(v) / len(v)\n",
    "results\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.ylabel(\"Precyzja klasyfikacji\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmiana promienia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
    "for j in range(100):\n",
    "    for i in range(1,20,1):\n",
    "        x_train_trimmed = X_train[:40][:]\n",
    "        y_train_trimmed = y_train[:40][:]\n",
    "        nc = RadiusNeighborsClassifier(radius = i)\n",
    "        model = nc.fit(x_train_trimmed, y_train_trimmed)\n",
    "        results[i] += [accuracy_score(y_test, nc.predict(X_test))]\n",
    "for k,v in results.items():\n",
    "    results[k] = sum(v) / len(v)\n",
    "results\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.ylabel(\"Precyzja klasyfikacji\")\n",
    "plt.xlabel(\"Radius\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmiana algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
    "for j in range(100):\n",
    "    for i in ['gaussian', 'tophat', 'epanechnikov', 'exponential','linear','cosine']:\n",
    "        x_train_trimmed = X_train[:100][:]\n",
    "        y_train_trimmed = y_train[:100][:]\n",
    "        nc = KernelDensity(kernel = i)\n",
    "        model = nc.fit(x_train_trimmed, y_train_trimmed)\n",
    "        results[i] += [nc.score(X_test)]\n",
    "for k,v in results.items():\n",
    "    results[k] = sum(v) / len(v)\n",
    "print(results)\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.ylabel(\"Precyzja klasyfikacji\")\n",
    "plt.xlabel(\"Kernel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wybieramy mały podzbiór - 100 elementów - danych ze zbioru cifar-10\n",
    "_, subsample_x, _, subsample_y = train_test_split(cifar10.data, cifar10.target, test_size=0.0016, stratify=cifar10.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcje pomocnicze\n",
    "def img_from_array(arr):\n",
    "    return arr.reshape(3, 32, 32).transpose(1,2,0).astype('uint8')\n",
    "\n",
    "def img_to_array(img):\n",
    "    return img.transpose(2,0,1).reshape(3 * 32 * 32)\n",
    "\n",
    "def display_imgs(*images):\n",
    "    f = plt.figure()\n",
    "    ind = 1\n",
    "    for img in images:\n",
    "        f.add_subplot(1, 2, ind)\n",
    "        plt.imshow(img)\n",
    "        ind += 1\n",
    "\n",
    "example = img_from_array(subsample_x[0])\n",
    "plt.imshow(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# możemy powiększyć zbiór danych np. odwracając obraz, nakładając szum...\n",
    "flipped_img = np.fliplr(example) # odwracanie horyzontalne\n",
    "\n",
    "def apply_noise(img): # gaussian noise\n",
    "    mean, var = 0, 50\n",
    "    sigma = var**0.5\n",
    "    gauss = np.random.normal(mean,sigma,img.shape)\n",
    "    return np.clip(gauss + img, 0, 255).astype(int)\n",
    "\n",
    "noisy_img = apply_noise(example)\n",
    "\n",
    "display_imgs(flipped_img, noisy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedura losowo powiększająca zbiór danych\n",
    "# z zadanym prawdopodobieństwem dodajemy zmodyfikowane obrazy do zbioru\n",
    "\n",
    "def augment(data, labels,flip_chance=0.2, noise_chance=0.1):\n",
    "    aug_data, aug_labels = [], []\n",
    "    for img, label in zip(data, labels):\n",
    "        new_img = None\n",
    "        if np.random.rand() < flip_chance:\n",
    "            new_img = np.fliplr(img_from_array(img))\n",
    "        if np.random.rand() < noise_chance:\n",
    "            new_img = apply_noise(new_img) if new_img is not None else apply_noise(img_from_array(img))\n",
    "\n",
    "        if new_img is not None:\n",
    "            aug_data.append(img_to_array(new_img))\n",
    "            aug_labels.append(label)\n",
    "\n",
    "    return (np.append(data, aug_data, 0), np.append(labels, aug_labels, 0))\n",
    "\n",
    "augmented_x, augmented_y = augment(subsample_x, subsample_y)\n",
    "\n",
    "print(f\"Added {len(augmented_x) - len(subsample_x)} new images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelujemy zachowanie miary jakości przykładowego klasyfikatora kolejno powiększając zbiór danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iters = 10\n",
    "scores = []\n",
    "data, labels = subsample_x, subsample_y\n",
    "original_size = len(data)\n",
    "for i in range(iters):\n",
    "    clf = KNeighborsClassifier()\n",
    "    res = cross_val_score(clf, data, labels)\n",
    "    scores.append((np.mean(res), len(data) - original_size))\n",
    "    data, labels = augment(data, labels)\n",
    "\n",
    "sc, num = list(zip(*scores))\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Number of augmented images')\n",
    "ax.set_ylabel('Cross validation score')\n",
    "ax.plot(num, sc, 'o')\n",
    "plt.show()\n",
    "print(f\"Original dataset size: {original_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 7\n",
    "### pełny spis dostępnych augmentatorów na https://imgaug.readthedocs.io/en/latest/source/overview_of_augmenters.html\n",
    "### git biblioteki https://github.com/aleju/imgaug/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as augs\n",
    "\n",
    "images = subsample_x.reshape(len(subsample_x), 3, 32, 32).transpose(0, 2, 3, 1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenters = [\n",
    "    augs.Fliplr(1), # horyzontalne odwrócenie obrazu\n",
    "    augs.CropAndPad(percent=(0, 0.1)), # przycięcie obrazu losowo 0-10% rozmiaru z dodanym paddingiem \n",
    "    augs.Rotate((-15, 15)), # obrócenie obrazu o -15 - 15 stopni\n",
    "    augs.Crop(px=(1, 16), keep_size=True), # przycięcie zdjęcia od 1 do 16 pikselów\n",
    "    augs.GaussianBlur(sigma=(0, 1.0)), # tak jak nazwa mówi\n",
    "    augs.Multiply((0.8, 1.2), per_channel=0.5), # zmiana jasności obrazu 80% - 120%\n",
    "    augs.EdgeDetect(alpha=(0, 0.7)), # wykrywanie krawędzi\n",
    "    augs.Affine(scale=(0.8, 1.2)) # skalowanie obrazu 80% - 120% \n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sekwencyjne aplikowanie augmentatorów z losową kolejnością\n",
    "seq = augs.Sequential(augmenters, random_order=True)\n",
    "image_aug = seq(images=images)\n",
    "\n",
    "display_imgs(images[0], image_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplikowanie 1-2 losowych augmentatorów\n",
    "some = augs.SomeOf((1, 2), augmenters)\n",
    "image_aug = some(images=images)\n",
    "\n",
    "display_imgs(images[0], image_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplikowanie na 30% obrazów\n",
    "sometimes = augs.Sometimes(0.3, augmenters)\n",
    "image_aug = sometimes(images=images)\n",
    "\n",
    "display_imgs(images[0], image_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplikowanie jednego z augmentatorów\n",
    "one = augs.OneOf(augmenters)\n",
    "image_aug = one(images=images)\n",
    "\n",
    "display_imgs(images[0], image_aug[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Zadanie do wykonania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonać punkty 6 i 7 na zbiorach MNIST, FMNIST przy pomocy imgaug oraz tworząc\n",
    "obrazki dodatkowe poprzez zaburzenie danych (np. dla x% pikseli losować liczbę 0,1 (MNIST), i\n",
    "odpowiednią – stopień szarości - dla zbioru FMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wczytaj dane MNIST, FMINST\n",
    "* Podziel dane na treningowe i testowe\n",
    "* Stwórz klasyfikator\n",
    "* Zbadaj dokładność klasyfikatora\n",
    "* Dokonaj augmentacji danych - imgaug, własna procedura\n",
    "* Porównaj miary jakości klasyfikatora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
